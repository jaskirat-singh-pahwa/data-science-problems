{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    " This is the problem of CUSTOMER CHURN;\n",
    " \n",
    " * You need to help out a marketing agency predict Customer Churn.\n",
    "\n",
    " * A marketing agency has many customers that use their service to produce ads \n",
    "    for clients/customer websites.\n",
    "\n",
    " * They have noticed that they have quite a bit of churn in clients.\n",
    "\n",
    " * They currently randomly assign account managers, but want you to create a \n",
    "   machine learning model that will help predict which customers will churn\n",
    "   (stop buying their service) so that they can correctly assign the customers\n",
    "   most at risk to churn an account manager.\n",
    "\n",
    " * Luckily they have some historical data, they want you to create a classification\n",
    "   algorithm that will help classify whether or not a customer churned.\n",
    "\n",
    " * Then the company can test this against incoming data for future customers\n",
    "   who will churn and assign them an account manager.\n",
    " \n",
    "\n",
    " * Attributes in the dataset:\n",
    "     \n",
    "     $ Name: Name of the latest contact at company.\n",
    "     $ Age: Customer age.\n",
    "     $ Total_Purchase: Total Ads purchased.\n",
    "     $ Account_Manager: Binary; 0 = No manager , 1 = Account manager assigned\n",
    "     $ Years: Total Years as a customer.\n",
    "     $ Num_sites: Number of websites that use the service.\n",
    "     $ Onboard_date: Date when the name of the latest contact was onboarded.\n",
    "     $ Location: Client HQ address.\n",
    "     $ Company: Name of client company.\n",
    "     \n",
    "     $ Churn: Target Column; 0 or 1 indicating whether a customer has churned or not. \n",
    "     \n",
    " * Remember that currently the account manager is randomly assigned,\n",
    "   So it won't be much helpful in the dataset to use in the algorithm.\n",
    "   \n",
    "'''\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"logRegConsult\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Names: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Total_Purchase: double (nullable = true)\n",
      " |-- Account_Manager: integer (nullable = true)\n",
      " |-- Years: double (nullable = true)\n",
      " |-- Num_Sites: double (nullable = true)\n",
      " |-- Onboard_date: timestamp (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Churn: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Lets Import the .csv file\n",
    "\n",
    "data = spark.read.csv(\"/Users/jaskiratsinghp/Desktop/Python-and-Spark-for-Big-Data-master/Spark_for_Machine_Learning/Logistic_Regression/customer_churn.csv\" , \n",
    "                     inferSchema = True , header = True)\n",
    "\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+-----------------+-----------------+------------------+-----------------+------------------+--------------------+--------------------+-------------------+\n",
      "|summary|        Names|              Age|   Total_Purchase|   Account_Manager|            Years|         Num_Sites|            Location|             Company|              Churn|\n",
      "+-------+-------------+-----------------+-----------------+------------------+-----------------+------------------+--------------------+--------------------+-------------------+\n",
      "|  count|          900|              900|              900|               900|              900|               900|                 900|                 900|                900|\n",
      "|   mean|         null|41.81666666666667|10062.82403333334|0.4811111111111111| 5.27315555555555| 8.587777777777777|                null|                null|0.16666666666666666|\n",
      "| stddev|         null|6.127560416916251|2408.644531858096|0.4999208935073339|1.274449013194616|1.7648355920350969|                null|                null| 0.3728852122772358|\n",
      "|    min|   Aaron King|             22.0|            100.0|                 0|              1.0|               3.0|00103 Jeffrey Cre...|     Abbott-Thompson|                  0|\n",
      "|    max|Zachary Walsh|             65.0|         18026.01|                 1|             9.15|              14.0|Unit 9800 Box 287...|Zuniga, Clark and...|                  1|\n",
      "+-------+-------------+-----------------+-----------------+------------------+-----------------+------------------+--------------------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Names='Cameron Williams', Age=42.0, Total_Purchase=11066.8, Account_Manager=0, Years=7.22, Num_Sites=8.0, Onboard_date=datetime.datetime(2013, 8, 30, 7, 0, 40), Location='10265 Elizabeth Mission Barkerburgh, AK 89518', Company='Harvey LLC', Churn=1)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Names',\n",
       " 'Age',\n",
       " 'Total_Purchase',\n",
       " 'Account_Manager',\n",
       " 'Years',\n",
       " 'Num_Sites',\n",
       " 'Onboard_date',\n",
       " 'Location',\n",
       " 'Company',\n",
       " 'Churn']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets convert the data in a way that is accepted by MLlib library.\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols = [\"Age\",\n",
    "                                        \"Total_Purchase\",\n",
    "                                        \"Account_Manager\",\n",
    "                                        \"Years\",\n",
    "                                        \"Num_Sites\"] , outputCol = \"features\")\n",
    "\n",
    "output = assembler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalData = output.select(\"features\" , \"churn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_churn , test_churn = finalData.randomSplit([0.7 , 0.3])\n",
    "\n",
    "## Lets import Logistic Regression library\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "logisticReg_churn = LogisticRegression(featuresCol = \"features\" , \n",
    "                                       labelCol = \"churn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fittedChurnModel = logisticReg_churn.fit(train_churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+\n",
      "|summary|              churn|         prediction|\n",
      "+-------+-------------------+-------------------+\n",
      "|  count|                622|                622|\n",
      "|   mean|0.14790996784565916|0.11093247588424437|\n",
      "| stddev| 0.3552964400597924|0.31430125748604026|\n",
      "|    min|                0.0|                0.0|\n",
      "|    max|                1.0|                1.0|\n",
      "+-------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainSummary = fittedChurnModel.summary\n",
    "\n",
    "trainSummary.predictions.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|churn|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|[26.0,8787.39,1.0...|    1|[0.43345902972804...|[0.60669934958693...|       0.0|\n",
      "|[28.0,9090.43,1.0...|    0|[1.33139100417859...|[0.79107063010370...|       0.0|\n",
      "|[28.0,11245.38,0....|    0|[3.45562572274596...|[0.96939846956511...|       0.0|\n",
      "|[29.0,5900.78,1.0...|    0|[3.64287878407945...|[0.97449087152586...|       0.0|\n",
      "|[30.0,6744.87,0.0...|    0|[2.98895226894464...|[0.95207252436678...|       0.0|\n",
      "|[30.0,8403.78,1.0...|    0|[5.32240203723663...|[0.99514269333554...|       0.0|\n",
      "|[30.0,8677.28,1.0...|    0|[3.93240948234531...|[0.98078023963537...|       0.0|\n",
      "|[30.0,10960.52,1....|    0|[2.25735630862558...|[0.90528318906589...|       0.0|\n",
      "|[30.0,12788.37,0....|    0|[2.17027240169814...|[0.89754801812296...|       0.0|\n",
      "|[30.0,13473.35,0....|    0|[2.36849406789370...|[0.91439305215226...|       0.0|\n",
      "|[31.0,5304.6,0.0,...|    0|[2.89780051726788...|[0.94773760147796...|       0.0|\n",
      "|[31.0,7073.61,0.0...|    0|[2.68825004430187...|[0.93632973529711...|       0.0|\n",
      "|[31.0,8688.21,0.0...|    0|[5.85139214891958...|[0.99713235434218...|       0.0|\n",
      "|[31.0,9574.89,0.0...|    0|[3.08163848232164...|[0.95612896437386...|       0.0|\n",
      "|[31.0,10182.6,1.0...|    0|[4.30107259833722...|[0.98662724134664...|       0.0|\n",
      "|[31.0,11297.57,1....|    1|[0.97553516222966...|[0.72622139717214...|       0.0|\n",
      "|[32.0,5756.12,0.0...|    0|[3.70778757255831...|[0.97605565834841...|       0.0|\n",
      "|[32.0,6367.22,1.0...|    0|[2.46342930455124...|[0.92153797992291...|       0.0|\n",
      "|[32.0,7896.65,0.0...|    0|[3.15426651550077...|[0.95907650562737...|       0.0|\n",
      "|[32.0,8011.38,0.0...|    0|[1.68401371168075...|[0.84343528150238...|       0.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Lets analyse some Evaluation matrices\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "predictionAndLabels = fittedChurnModel.evaluate(test_churn)\n",
    "\n",
    "predictionAndLabels.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "churnEval = BinaryClassificationEvaluator(rawPredictionCol = \"prediction\",\n",
    "                                         labelCol = \"churn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUC = churnEval.evaluate(predictionAndLabels.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7536050156739812"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets try to predict on New Dataset\n",
    "\n",
    "final_lr_model = logisticReg_churn.fit(finalData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Names: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Total_Purchase: double (nullable = true)\n",
      " |-- Account_Manager: integer (nullable = true)\n",
      " |-- Years: double (nullable = true)\n",
      " |-- Num_Sites: double (nullable = true)\n",
      " |-- Onboard_date: timestamp (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newCustomerData = spark.read.csv(\"/Users/jaskiratsinghp/Desktop/Python-and-Spark-for-Big-Data-master/Spark_for_Machine_Learning/Logistic_Regression/new_customers.csv\" , \n",
    "                                inferSchema = True , header = True)\n",
    "\n",
    "newCustomerData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "testNewCustomers = assembler.transform(newCustomerData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Names: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- Total_Purchase: double (nullable = true)\n",
      " |-- Account_Manager: integer (nullable = true)\n",
      " |-- Years: double (nullable = true)\n",
      " |-- Num_Sites: double (nullable = true)\n",
      " |-- Onboard_date: timestamp (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testNewCustomers.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalResults = final_lr_model.transform(testNewCustomers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+\n",
      "|prediction|         Company|\n",
      "+----------+----------------+\n",
      "|       0.0|        King Ltd|\n",
      "|       1.0|   Cannon-Benson|\n",
      "|       1.0|Barron-Robertson|\n",
      "|       1.0|   Sexton-Golden|\n",
      "|       0.0|        Wood LLC|\n",
      "|       1.0|   Parks-Robbins|\n",
      "+----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finalResults.select(\"prediction\" , \"Company\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
