{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This dataset has features of universities and labeled either \n",
    "Private or Public.\n",
    "\n",
    "We will use different tree methods to attempt to label or build model\n",
    "that can predict whether university based on features is private \n",
    "or public.\n",
    "\n",
    "'''\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"tree\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- School: string (nullable = true)\n",
      " |-- Private: string (nullable = true)\n",
      " |-- Apps: integer (nullable = true)\n",
      " |-- Accept: integer (nullable = true)\n",
      " |-- Enroll: integer (nullable = true)\n",
      " |-- Top10perc: integer (nullable = true)\n",
      " |-- Top25perc: integer (nullable = true)\n",
      " |-- F_Undergrad: integer (nullable = true)\n",
      " |-- P_Undergrad: integer (nullable = true)\n",
      " |-- Outstate: integer (nullable = true)\n",
      " |-- Room_Board: integer (nullable = true)\n",
      " |-- Books: integer (nullable = true)\n",
      " |-- Personal: integer (nullable = true)\n",
      " |-- PhD: integer (nullable = true)\n",
      " |-- Terminal: integer (nullable = true)\n",
      " |-- S_F_Ratio: double (nullable = true)\n",
      " |-- perc_alumni: integer (nullable = true)\n",
      " |-- Expend: integer (nullable = true)\n",
      " |-- Grad_Rate: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Lets load the data\n",
    "\n",
    "data = spark.read.csv(\"/Users/jaskiratsinghp/Desktop/Python-and-Spark-for-Big-Data-master/Spark_for_Machine_Learning/Tree_Methods/College.csv\" , \n",
    "                      inferSchema = True , header = True)\n",
    "\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(School='Abilene Christian University', Private='Yes', Apps=1660, Accept=1232, Enroll=721, Top10perc=23, Top25perc=52, F_Undergrad=2885, P_Undergrad=537, Outstate=7440, Room_Board=3300, Books=450, Personal=2200, PhD=70, Terminal=78, S_F_Ratio=18.1, perc_alumni=12, Expend=7041, Grad_Rate=60)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['School',\n",
       " 'Private',\n",
       " 'Apps',\n",
       " 'Accept',\n",
       " 'Enroll',\n",
       " 'Top10perc',\n",
       " 'Top25perc',\n",
       " 'F_Undergrad',\n",
       " 'P_Undergrad',\n",
       " 'Outstate',\n",
       " 'Room_Board',\n",
       " 'Books',\n",
       " 'Personal',\n",
       " 'PhD',\n",
       " 'Terminal',\n",
       " 'S_F_Ratio',\n",
       " 'perc_alumni',\n",
       " 'Expend',\n",
       " 'Grad_Rate']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets convert the dataset into desired format\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols = ['Apps',\n",
    "                                         'Accept',\n",
    "                                         'Enroll',\n",
    "                                         'Top10perc',\n",
    "                                         'Top25perc',\n",
    "                                         'F_Undergrad',\n",
    "                                         'P_Undergrad',\n",
    "                                         'Outstate',\n",
    "                                         'Room_Board',\n",
    "                                         'Books',\n",
    "                                         'Personal',\n",
    "                                         'PhD',\n",
    "                                         'Terminal',\n",
    "                                         'S_F_Ratio',\n",
    "                                         'perc_alumni',\n",
    "                                         'Expend',\n",
    "                                         'Grad_Rate'] , outputCol = \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = assembler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Lets change the output column as it is string column that has values\n",
    "either Yes or No, we have to convert that into integers.\n",
    "\n",
    "'''\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol = \"Private\" , outputCol = \"PrivateIndex\")\n",
    "\n",
    "outputFixed = indexer.fit(output).transform(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- School: string (nullable = true)\n",
      " |-- Private: string (nullable = true)\n",
      " |-- Apps: integer (nullable = true)\n",
      " |-- Accept: integer (nullable = true)\n",
      " |-- Enroll: integer (nullable = true)\n",
      " |-- Top10perc: integer (nullable = true)\n",
      " |-- Top25perc: integer (nullable = true)\n",
      " |-- F_Undergrad: integer (nullable = true)\n",
      " |-- P_Undergrad: integer (nullable = true)\n",
      " |-- Outstate: integer (nullable = true)\n",
      " |-- Room_Board: integer (nullable = true)\n",
      " |-- Books: integer (nullable = true)\n",
      " |-- Personal: integer (nullable = true)\n",
      " |-- PhD: integer (nullable = true)\n",
      " |-- Terminal: integer (nullable = true)\n",
      " |-- S_F_Ratio: double (nullable = true)\n",
      " |-- perc_alumni: integer (nullable = true)\n",
      " |-- Expend: integer (nullable = true)\n",
      " |-- Grad_Rate: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- PrivateIndex: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "outputFixed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalData = outputFixed.select(\"features\" , \"PrivateIndex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets split the dataset into train and test dataset.\n",
    "\n",
    "train_data , test_data = finalData.randomSplit([0.7 , 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets import the classifiers\n",
    "\n",
    "from pyspark.ml.classification import (DecisionTreeClassifier , \n",
    "                                       GBTClassifier , \n",
    "                                       RandomForestClassifier)\n",
    "\n",
    "#from pyspark.ml.regression import RandomForestRegressor , etc..\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "decisionTreeClassifier = DecisionTreeClassifier(featuresCol = \"features\", \n",
    "                                               labelCol = \"PrivateIndex\")\n",
    "\n",
    "randomForestClassifier = RandomForestClassifier(featuresCol = \"features\",\n",
    "                                                labelCol = \"PrivateIndex\")\n",
    "\n",
    "gradientBoostingClassifier = GBTClassifier(featuresCol = \"features\" , \n",
    "                                           labelCol = \"PrivateIndex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets fit the model\n",
    "\n",
    "decisionTreeModel = decisionTreeClassifier.fit(train_data)\n",
    "\n",
    "randomForestModel = randomForestClassifier.fit(train_data)\n",
    "\n",
    "gradientBoostingModel = gradientBoostingClassifier.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets do some predictions on test data\n",
    "\n",
    "decisionTreePredictions = decisionTreeModel.transform(test_data)\n",
    "\n",
    "randomForestPredictions = randomForestModel.transform(test_data)\n",
    "\n",
    "gradientBoostingPredictions = gradientBoostingModel.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets see evaluation matrices\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "myBinaryEval = BinaryClassificationEvaluator(labelCol = \"PrivateIndex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy:  0.9250021916367142\n",
      "\n",
      "Random Forest Accuracy:  0.9832558955027619\n",
      "\n",
      "Gradient Boosting Accuracy:  0.948978697291137\n"
     ]
    }
   ],
   "source": [
    "print(\"Decision Tree Accuracy: \" , myBinaryEval.evaluate(decisionTreePredictions))\n",
    "\n",
    "print(\"\\nRandom Forest Accuracy: \" , myBinaryEval.evaluate(randomForestPredictions))\n",
    "\n",
    "print(\"\\nGradient Boosting Accuracy: \" , myBinaryEval.evaluate(gradientBoostingPredictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "If you want to test things like Accuracy , Precision or Recall,\n",
    "you can grab those DIRECTLY from BinaryClassificationEvaluator.\n",
    "\n",
    "But, you can't grab those from MulticlassClassificationEvaluator DIRECTLY.\n",
    "\n",
    "'''\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "accEval = MulticlassClassificationEvaluator(labelCol = \"PrivateIndex\" , \n",
    "                                            metricName = \"accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9395161290322581"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomForestAccuracy = accEval.evaluate(randomForestPredictions)\n",
    "\n",
    "randomForestAccuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest - Classification Consulting     Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "There is this Dog Food Company in St.Louis, Missouri.\n",
    "\n",
    "* The company is trying to predict why some batches of their dog food\n",
    "  are spoiling much quicker than intended.\n",
    "\n",
    "* Unfortunately, this Dog Food company has not upgraded to the latest \n",
    "  machinery, meaning that the amounts of the 5 preservative chemicals\n",
    "  they are using can vary a lot, but WE HAVE TO PREDICT THAT WHICH \n",
    "  CHEMICAL HAS STRONGEST EFFECT.\n",
    "  \n",
    "* The Dog Food company first mixes up a batch of preservative that\n",
    "  contains 4 different preservative chemicals (A, B, C, D) and then\n",
    "  is completed with a \"filler\" chemical.\n",
    "\n",
    "* The Food Scientists believe one of the A, B, C or D preservatives\n",
    "  is causing the problem, but need your help to figure out which one.\n",
    "  \n",
    "* Use Machine Learning with RF to find out which parameter had the\n",
    "  most predictive power, thus finding out which chemical causes the \n",
    "  early spoiling.\n",
    "\n",
    "* So create a model and then find out how you can decide which chemical\n",
    "  is the problem.\n",
    "  \n",
    "\n",
    "Features in the dataset:\n",
    "\n",
    "1.) Pres_A: Percentage of preservative A in the mix.\n",
    "2.) Pres_B: Percentage of preservative B in the mix.\n",
    "3.) Pres_C: Percentage of preservative C in the mix.\n",
    "4.) Pres_D: Percentage of preservative D in the mix.\n",
    "5.) Spoiled: Label indicating whether or not the Dog Food batch was spoiled.\n",
    "\n",
    "\n",
    "THERE ARE MANY DIFFERENT WAYS TO SOLVE THIS PROBLEM BUT, WE WILL SOLVE\n",
    "THIS USING STATISTICS.\n",
    "\n",
    "$ It won't be like a typical workflow, we won't be working with train_data\n",
    "  and test_data, instead we will feed the model the complete dataset and \n",
    "  using feature_importance functionality we will find out which feature \n",
    "  is causing food to get spoiled.\n",
    "  \n",
    "'''\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"RF_consult\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- A: integer (nullable = true)\n",
      " |-- B: integer (nullable = true)\n",
      " |-- C: double (nullable = true)\n",
      " |-- D: integer (nullable = true)\n",
      " |-- Spoiled: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Lets read the data\n",
    "\n",
    "data = spark.read.csv(\"/Users/jaskiratsinghp/Desktop/Python-and-Spark-for-Big-Data-master/Spark_for_Machine_Learning/Tree_Methods/dog_food.csv\" , \n",
    "                      inferSchema = True , header = True)\n",
    "\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+---+-------+\n",
      "|  A|  B|   C|  D|Spoiled|\n",
      "+---+---+----+---+-------+\n",
      "|  4|  2|12.0|  3|    1.0|\n",
      "|  5|  6|12.0|  7|    1.0|\n",
      "|  6|  2|13.0|  6|    1.0|\n",
      "|  4|  2|12.0|  1|    1.0|\n",
      "|  4|  2|12.0|  3|    1.0|\n",
      "| 10|  3|13.0|  9|    1.0|\n",
      "|  8|  5|14.0|  5|    1.0|\n",
      "|  5|  8|12.0|  8|    1.0|\n",
      "|  6|  5|12.0|  9|    1.0|\n",
      "|  3|  3|12.0|  1|    1.0|\n",
      "|  9|  8|11.0|  3|    1.0|\n",
      "|  1| 10|12.0|  3|    1.0|\n",
      "|  1|  5|13.0| 10|    1.0|\n",
      "|  2| 10|12.0|  6|    1.0|\n",
      "|  1| 10|11.0|  4|    1.0|\n",
      "|  5|  3|12.0|  2|    1.0|\n",
      "|  4|  9|11.0|  8|    1.0|\n",
      "|  5|  1|11.0|  1|    1.0|\n",
      "|  4|  9|12.0| 10|    1.0|\n",
      "|  5|  8|10.0|  9|    1.0|\n",
      "+---+---+----+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(A=4, B=2, C=12.0, D=3, Spoiled=1.0)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'B', 'C', 'D', 'Spoiled']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets create an assembler.\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(inputCols = [\"A\" , \"B\" , \"C\" , \"D\"] , \n",
    "                            outputCol = \"features\")\n",
    "\n",
    "output = assembler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "randomForestClassifier = RandomForestClassifier(featuresCol = \"features\" , \n",
    "                                                labelCol = \"Spoiled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- A: integer (nullable = true)\n",
      " |-- B: integer (nullable = true)\n",
      " |-- C: double (nullable = true)\n",
      " |-- D: integer (nullable = true)\n",
      " |-- Spoiled: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalData = output.select(\"features\" , \"Spoiled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------+\n",
      "|           features|Spoiled|\n",
      "+-------------------+-------+\n",
      "| [4.0,2.0,12.0,3.0]|    1.0|\n",
      "| [5.0,6.0,12.0,7.0]|    1.0|\n",
      "| [6.0,2.0,13.0,6.0]|    1.0|\n",
      "| [4.0,2.0,12.0,1.0]|    1.0|\n",
      "| [4.0,2.0,12.0,3.0]|    1.0|\n",
      "|[10.0,3.0,13.0,9.0]|    1.0|\n",
      "| [8.0,5.0,14.0,5.0]|    1.0|\n",
      "| [5.0,8.0,12.0,8.0]|    1.0|\n",
      "| [6.0,5.0,12.0,9.0]|    1.0|\n",
      "| [3.0,3.0,12.0,1.0]|    1.0|\n",
      "| [9.0,8.0,11.0,3.0]|    1.0|\n",
      "|[1.0,10.0,12.0,3.0]|    1.0|\n",
      "|[1.0,5.0,13.0,10.0]|    1.0|\n",
      "|[2.0,10.0,12.0,6.0]|    1.0|\n",
      "|[1.0,10.0,11.0,4.0]|    1.0|\n",
      "| [5.0,3.0,12.0,2.0]|    1.0|\n",
      "| [4.0,9.0,11.0,8.0]|    1.0|\n",
      "| [5.0,1.0,11.0,1.0]|    1.0|\n",
      "|[4.0,9.0,12.0,10.0]|    1.0|\n",
      "| [5.0,8.0,10.0,9.0]|    1.0|\n",
      "+-------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finalData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomForestModel = randomForestClassifier.fit(finalData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(4, {0: 0.0177, 1: 0.0199, 2: 0.9481, 3: 0.0142})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomForestModel.featureImportances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Outcome:\n",
    "### Therefore, we can see that Preservative_C is causing the Spoiling of food."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
